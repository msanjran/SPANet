{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9097636d-5bc8-44b9-a7da-c4ce66f45bfc",
   "metadata": {},
   "source": [
    "# View Training Metrics\n",
    "\n",
    "**Author: Shahzad Sanjrani**\n",
    "\n",
    "**Date: 21.11.24**\n",
    "\n",
    "This is a way to quickly produce training metrics for a specific `version_x` for experimental/test purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30258446-9869-4328-8740-579ee5194662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# ignore ugly ass warning --> doesn't work...\n",
    "with warnings.catch_warnings(action=\"ignore\"):\n",
    "    from tensorflow.python.summary.summary_iterator import summary_iterator\n",
    "    \n",
    "import os\n",
    "import pandas as pd\n",
    "import mplhep as hep\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a61edf09-8b23-4785-be4c-e979be94725e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tb_data(root_dir, sort_by=None):\n",
    "    \"\"\"Convert local TensorBoard data into Pandas DataFrame.\n",
    "    \n",
    "    Function takes the root directory path and recursively parses\n",
    "    all events data.    \n",
    "    If the `sort_by` value is provided then it will use that column\n",
    "    to sort values; typically `wall_time` or `step`.\n",
    "    \n",
    "    *Note* that the whole data is converted into a DataFrame.\n",
    "    Depending on the data size this might take a while. If it takes\n",
    "    too long then narrow it to some sub-directories.\n",
    "    \n",
    "    Paramters:\n",
    "        root_dir: (str) path to root dir with tensorboard data.\n",
    "        sort_by: (optional str) column name to sort by.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame with [wall_time, name, step, value] columns.\n",
    "\n",
    "    Note: originally from https://laszukdawid.com/blog/2021/01/26/parsing-tensorboard-data-locally/\n",
    "    \"\"\"\n",
    "\n",
    "    def convert_tfevent(filepath):\n",
    "        return pd.DataFrame([\n",
    "            parse_tfevent(e) for e in summary_iterator(filepath) if len(e.summary.value)\n",
    "        ])\n",
    "\n",
    "    def parse_tfevent(tfevent):\n",
    "        return dict(\n",
    "            wall_time=tfevent.wall_time,\n",
    "            name=tfevent.summary.value[0].tag,\n",
    "            step=tfevent.step,\n",
    "            value=float(tfevent.summary.value[0].simple_value),\n",
    "        )\n",
    "    \n",
    "    columns_order = ['wall_time', 'name', 'step', 'value']\n",
    "    \n",
    "    out = []\n",
    "    # originally os.walk but i don't see the point...\n",
    "    for filename in glob.glob(f\"{root_dir}/*\"):\n",
    "        if \"events.out.tfevents\" not in filename:\n",
    "            continue\n",
    "        file_full_path = os.path.join(filename)\n",
    "        out.append(convert_tfevent(file_full_path))\n",
    "\n",
    "    # Concatenate (and sort) all partial individual dataframes\n",
    "    all_df = pd.concat(out)[columns_order]\n",
    "    if sort_by is not None:\n",
    "        all_df = all_df.sort_values(sort_by)\n",
    "        \n",
    "    return all_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefd7cd6-3d3e-471a-929e-4011df3c6320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hepfig(figsize=(10,10)):\n",
    "    ''' plt.figure but use the mplhep package to make it look nice '''\n",
    "    hep.style.use(\"CMS\")\n",
    "    mpl.rcParams[\"font.size\"] = 20\n",
    "    plt.figure(figsize=figsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "653711fc-37d1-429b-982a-d61b0fa14905",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_output(df, metric, extra=None, save = None, verbose=False):\n",
    "    '''\n",
    "    Given metric in df, plot metric\n",
    "    '''\n",
    "    metric_arr = df[ df['name'] == metric ]['value']\n",
    "    x_arr = df[ df['name'] == metric ]['step']\n",
    "\n",
    "    plot_hepfig()\n",
    "    plt.plot(x_arr, metric_arr, label=metric if extra is None else f\"{extra}_{metric}\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    if save is not None:\n",
    "        os.makedirs(save, exist_ok=True)\n",
    "        metric_name = metric.replace(\"/\", \"-\") # otherwise tedious\n",
    "        fpath = os.path.join(save, f\"{metric_name}.pdf\")\n",
    "        plt.savefig(fpath)\n",
    "        plt.close()\n",
    "        if verbose: print(f\"saved {fpath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f43c64-5b0a-45ad-abcf-714aa79b9bcc",
   "metadata": {},
   "source": [
    "## User input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e194ba52-d0ed-48fa-83e3-39d62a80a6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dir = f\"/nfs/dust/cms/user/sanjrani/SPANet_Investigations/investigation2/pepper_analysis/output/h4t_systematics/spanet/models/spanet_output\"\n",
    "model = f\"version_5\"\n",
    "\n",
    "df = convert_tb_data(os.path.join(store_dir, model))\n",
    "# print(df['name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfd456bd-7ab5-417a-8a68-8c27ff14cb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ideas:\n",
    "# 'loss/zpt1/assignment_loss' 'loss/zpt2/assignment_loss'\n",
    "# 'loss/nzpt1/assignment_loss' 'loss/nzpt2/assignment_loss'\n",
    "# 'loss/zpt1/detection_loss' 'loss/zpt2/detection_loss'\n",
    "# 'loss/nzpt1/detection_loss' 'loss/nzpt2/detection_loss' 'loss/total_loss'\n",
    "# 'Purity/*tzp*nztp/event_purity' 'validation_accuracy'\n",
    "# 'Purity/*tzp*nztp/event_proportion' 'Purity/*tzp*nztp/tzp_purity'\n",
    "# 'Purity/*tzp*nztp/nztp_purity' 'Purity/*tzp0nztp/event_purity'\n",
    "# 'Purity/*tzp0nztp/event_proportion' 'Purity/*tzp0nztp/tzp_purity'\n",
    "# 'Purity/*tzp1nztp/event_purity' 'Purity/*tzp1nztp/event_proportion'\n",
    "# 'Purity/*tzp1nztp/tzp_purity' 'Purity/*tzp1nztp/nztp_purity'\n",
    "# 'Purity/*tzp2nztp/event_purity' 'Purity/*tzp2nztp/event_proportion'\n",
    "# 'Purity/*tzp2nztp/tzp_purity' 'Purity/*tzp2nztp/nztp_purity'\n",
    "# 'Purity/0tzp*nztp/event_purity' 'Purity/0tzp*nztp/event_proportion'\n",
    "# 'Purity/0tzp*nztp/nztp_purity' 'Purity/0tzp0nztp/event_purity'\n",
    "# 'Purity/0tzp0nztp/event_proportion' 'Purity/0tzp1nztp/event_purity'\n",
    "# 'Purity/0tzp1nztp/event_proportion' 'Purity/0tzp1nztp/nztp_purity'\n",
    "# 'Purity/0tzp2nztp/event_purity' 'Purity/0tzp2nztp/event_proportion'\n",
    "# 'Purity/0tzp2nztp/nztp_purity' 'Purity/1tzp*nztp/event_purity'\n",
    "# 'Purity/1tzp*nztp/event_proportion' 'Purity/1tzp*nztp/tzp_purity'\n",
    "# 'Purity/1tzp*nztp/nztp_purity' 'Purity/1tzp0nztp/event_purity'\n",
    "# 'Purity/1tzp0nztp/event_proportion' 'Purity/1tzp0nztp/tzp_purity'\n",
    "# 'Purity/1tzp1nztp/event_purity' 'Purity/1tzp1nztp/event_proportion'\n",
    "# 'Purity/1tzp1nztp/tzp_purity' 'Purity/1tzp1nztp/nztp_purity'\n",
    "# 'Purity/1tzp2nztp/event_purity' 'Purity/1tzp2nztp/event_proportion'\n",
    "# 'Purity/1tzp2nztp/tzp_purity' 'Purity/1tzp2nztp/nztp_purity'\n",
    "# 'Purity/2tzp*nztp/event_purity' 'Purity/2tzp*nztp/event_proportion'\n",
    "# 'Purity/2tzp*nztp/tzp_purity' 'Purity/2tzp*nztp/nztp_purity'\n",
    "# 'Purity/2tzp0nztp/event_purity' 'Purity/2tzp0nztp/event_proportion'\n",
    "# 'Purity/2tzp0nztp/tzp_purity' 'Purity/2tzp1nztp/event_purity'\n",
    "# 'Purity/2tzp1nztp/event_proportion' 'Purity/2tzp1nztp/tzp_purity'\n",
    "# 'Purity/2tzp1nztp/nztp_purity' 'Purity/2tzp2nztp/event_purity'\n",
    "# 'Purity/2tzp2nztp/event_proportion' 'Purity/2tzp2nztp/tzp_purity'\n",
    "# 'Purity/2tzp2nztp/nztp_purity' 'jet/accuracy_1_of_1'\n",
    "# 'jet/accuracy_1_of_2' 'jet/accuracy_2_of_2' 'jet/accuracy_1_of_3'\n",
    "# 'jet/accuracy_2_of_3' 'jet/accuracy_3_of_3' 'jet/accuracy_1_of_4'\n",
    "# 'jet/accuracy_2_of_4' 'jet/accuracy_3_of_4' 'jet/accuracy_4_of_4'\n",
    "# 'particle/accuracy_1_of_1' 'particle/accuracy_1_of_2'\n",
    "# 'particle/accuracy_2_of_2' 'particle/accuracy_1_of_3'\n",
    "# 'particle/accuracy_2_of_3' 'particle/accuracy_3_of_3'\n",
    "# 'particle/accuracy_1_of_4' 'particle/accuracy_2_of_4'\n",
    "# 'particle/accuracy_3_of_4' 'particle/accuracy_4_of_4' 'particle/accuracy'\n",
    "# 'particle/sensitivity' 'particle/specificity' 'particle/f_score'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef45d78-a461-4e53-84a7-121c995f96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some ideas:\n",
    "#  'loss/t1/assignment_loss' 'loss/t2/assignment_loss'\n",
    "#  'loss/t3/assignment_loss' 'loss/t4/assignment_loss'\n",
    "#  'loss/t1/detection_loss' 'loss/t2/detection_loss'\n",
    "#  'loss/t3/detection_loss' 'loss/t4/detection_loss' 'loss/total_loss'\n",
    "#  'epoch' 'Purity/*t/event_purity' 'Purity/*t/event_proportion'\n",
    "#  'Purity/*t/t_purity' 'Purity/0t/event_purity'\n",
    "#  'Purity/0t/event_proportion' 'Purity/1t/event_purity'\n",
    "#  'Purity/1t/event_proportion' 'Purity/1t/t_purity'\n",
    "#  'Purity/2t/event_purity' 'Purity/2t/event_proportion'\n",
    "#  'Purity/2t/t_purity' 'Purity/3t/event_purity'\n",
    "#  'Purity/3t/event_proportion' 'Purity/3t/t_purity'\n",
    "#  'Purity/4t/event_purity' 'Purity/4t/event_proportion'\n",
    "#  'Purity/4t/t_purity' 'jet/accuracy_1_of_1' 'jet/accuracy_1_of_2'\n",
    "#  'jet/accuracy_2_of_2' 'jet/accuracy_1_of_3' 'jet/accuracy_2_of_3'\n",
    "#  'jet/accuracy_3_of_3' 'jet/accuracy_1_of_4' 'jet/accuracy_2_of_4'\n",
    "#  'jet/accuracy_3_of_4' 'jet/accuracy_4_of_4' 'particle/accuracy_1_of_1'\n",
    "#  'particle/accuracy_1_of_2' 'particle/accuracy_2_of_2'\n",
    "#  'particle/accuracy_1_of_3' 'particle/accuracy_2_of_3'\n",
    "#  'particle/accuracy_3_of_3' 'particle/accuracy_1_of_4'\n",
    "#  'particle/accuracy_2_of_4' 'particle/accuracy_3_of_4'\n",
    "#  'particle/accuracy_4_of_4' 'particle/accuracy' 'particle/sensitivity'\n",
    "#  'particle/specificity' 'particle/f_score' 'validation_accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51276ef7-882e-4bc2-854c-7a68788f3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join(store_dir, model, \"training_metrics\")\n",
    "plot_output(df, 'jet/accuracy_2_of_4', save = save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b96b8b-89a2-4649-ab1d-87db7a60add5",
   "metadata": {},
   "source": [
    "## Quick and dirty\n",
    "\n",
    "Plot all the relevant metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3f1a9bb2-6f92-4307-9f5e-b3f4f9856280",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 534/534 [00:08<00:00, 64.70it/s]  \n"
     ]
    }
   ],
   "source": [
    "save_dir = os.path.join(store_dir, model, \"training_metrics\")\n",
    "\n",
    "metrics_filter = ['jet', 'Purity', 'loss', 'particle', 'accuracy']\n",
    "training_metrics = []\n",
    "\n",
    "for name in tqdm(df['name'].unique()):\n",
    "\n",
    "    for mfilter in metrics_filter:\n",
    "        if mfilter in name:\n",
    "            training_metrics.append(name)\n",
    "            plot_output(df, name, save = save_dir, extra=model, verbose=False)\n",
    "            break\n",
    "\n",
    "# print(training_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb60620-f482-48e0-807a-d5ae6f7628b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "useTensorboard",
   "language": "python",
   "name": "usetensorboard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
